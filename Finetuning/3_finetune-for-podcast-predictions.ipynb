{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":413,"referenced_widgets":["97dbf73c71bf474eb7ead8eea3de267e","b0ebb476e2d34031b163201391834d55","62af37b141a24d239405468935631f4d","f6d28125c84d49bc8ceab33cd62c0fc3","9a164efbd36742c9bfd0e1590b9e4404","42424c9e84ac42849d3b965a1a4a3e2d","1013f37ce761413b8c9ac2d8c62d694b","41b2f3ce15f045fc8da36780acfcae7a","15034e482f1c449089c230632aba7752","eb6bd245c4d0471a963d5e99239f0339","be30e13e270b486f94d455ae3aa07cfd","7d74dcab889a497a8bf60b3c95003136","d7f917bfa52c49d1847b5ef633011f7d","272fb4f048f744729f4dbc3930913c70","ecbcaeea24bd4dcda15a83f60a0c613f","fd9e295ff6a042538eeefb157616a52e","40a532dbba774d308134391e0afbf7a5","97eee52eba864ba3a5b9944cb99abec9","67a437da6b7c473a996d148e9f36187b","a080270963634a03b3363411aa17c268","97bd86231e16443483d3ecf85e1e8e6b","39a7d683f8fa4715891e463ff270563d"]},"executionInfo":{"elapsed":194597,"status":"ok","timestamp":1761874302857,"user":{"displayName":"khaled Aly","userId":"04673088304537721042"},"user_tz":240},"id":"ExJ0o37beDwI","outputId":"6ec0b2b6-96f4-44d6-f993-c8fc9f96d4fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚ö†Ô∏è Hugging Face token file not found. This may cause issues.\n","Base model: Qwen/Qwen3-0.6B\n","Adapter: qwen_qlora_podcast\n","Output directory: qwen_qlora_podcast_merged\n","\n","1. Loading base model in 4-bit...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97dbf73c71bf474eb7ead8eea3de267e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d74dcab889a497a8bf60b3c95003136","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","2. Loading PEFT model (adapter)...\n","\n","3. Merging adapter into the base model...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["‚úÖ Merge complete.\n","\n","4. Saving merged model to qwen_qlora_podcast_merged...\n","\n","üéâ Merged model saved successfully to qwen_qlora_podcast_merged\n","You can now load this model directly for fast inference.\n"]}],"source":["#### When running the inference on Qwen 0.6 with the 4bit quantization, each example\n","#### Takes about 65 seconds to run. to speed up the inference, this code merges the model with the adapter\n","#### Merging helped reduce the time from 65 seconds per prompt to 40 seconds\n","#### After further investigation, i can see that the time taken is primeraly from prefilling (long prompt)\n","\n","# !pip install bitsandbytes     ## Needed when training on Colab\n","import os\n","import torch\n","from peft import PeftModel\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","\n","# --- Configuration ---\n","BASE_MODEL_PATH = \"Qwen/Qwen3-0.6B\"\n","ADAPTER_PATH = \"qwen_qlora_podcast\"  # Path to your fine-tuned adapter\n","MERGED_MODEL_OUTPUT_PATH = \"qwen_qlora_podcast_merged\"\n","\n","# Get Hugging Face token\n","HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n","if not HF_TOKEN:\n","    try:\n","        with open(\"../../../secrets/hf_token.txt\", \"r\") as f:\n","            HF_TOKEN = f.readline().strip()\n","    except FileNotFoundError:\n","        print(\"‚ö†Ô∏è Hugging Face token file not found. This may cause issues.\")\n","        HF_TOKEN = None\n","\n","def merge_qlora_model():\n","    \"\"\"Merges a QLoRA adapter into the base model and saves the full model.\"\"\"\n","    print(f\"Base model: {BASE_MODEL_PATH}\")\n","    print(f\"Adapter: {ADAPTER_PATH}\")\n","    print(f\"Output directory: {MERGED_MODEL_OUTPUT_PATH}\")\n","\n","    # 1. Load the base model with 4-bit quantization\n","    print(\"\\n1. Loading base model in 4-bit...\")\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","    )\n","    base_model = AutoModelForCausalLM.from_pretrained(\n","        BASE_MODEL_PATH,\n","        quantization_config=bnb_config,\n","        trust_remote_code=True,\n","        token=HF_TOKEN,\n","        device_map=\"cpu\",  # Load on CPU to avoid VRAM issues during merge\n","    )\n","\n","    # 2. Load the PEFT model (applying the adapter)\n","    print(\"\\n2. Loading PEFT model (adapter)...\")\n","    peft_model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n","\n","    # 3. Merge the adapter into the base model\n","    print(\"\\n3. Merging adapter into the base model...\")\n","    merged_model = peft_model.merge_and_unload()\n","    print(\"‚úÖ Merge complete.\")\n","\n","    # 4. Save the merged model and tokenizer\n","    print(f\"\\n4. Saving merged model to {MERGED_MODEL_OUTPUT_PATH}...\")\n","    os.makedirs(MERGED_MODEL_OUTPUT_PATH, exist_ok=True)\n","    merged_model.save_pretrained(MERGED_MODEL_OUTPUT_PATH)\n","\n","    # Also save the tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH, trust_remote_code=True, token=HF_TOKEN)\n","    tokenizer.save_pretrained(MERGED_MODEL_OUTPUT_PATH)\n","\n","    print(f\"\\nüéâ Merged model saved successfully to {MERGED_MODEL_OUTPUT_PATH}\")\n","    print(\"You can now load this model directly for fast inference.\")\n","\n","if __name__ == \"__main__\":\n","    merge_qlora_model()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":986,"referenced_widgets":["d8b4bd488dc8457a8c3d3906d5a61250","51441de6edf94216883e4979e5396fba","ccb50a2924374d29b28eef5fb04cec4d","1265ea30930e46d8ba0d21f480024019","e06087e7699f4681bb3ca0d47b07f108","13b199d3bcad48ddb206ac8a5b367e59","246c3266bef34b2c9dc261ab9be2deb6","79f97ef43b9a460b9bc5a95a5e3534f3","72942c45b88244fb81d2f1b5bf8d6150","19ca0f4bebb14dfebcbd95ecaf97b0ef","f96c0aa2ce354b4b91da3443ebd8d56b"]},"id":"eC4thAKWWLUO","outputId":"6dd20bb9-9dad-4417-cf59-ff4a102d28c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluating Merged Fine-tuned Qwen Model\n"]},{"name":"stderr","output_type":"stream","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"name":"stdout","output_type":"stream","text":["Found 2000 examples to evaluate\n","\n","üöÄ Loading merged model from qwen_qlora_podcast_merged...\n","Model and adapter loaded successfully\n","Starting inference...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d8b4bd488dc8457a8c3d3906d5a61250","version_major":2,"version_minor":0},"text/plain":["Inferencing:   0%|          | 0/2000 [00:00<?, ?example/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["‚úÖ Processed 1/2000 examples\n","‚úÖ Processed 2/2000 examples\n","‚úÖ Processed 3/2000 examples\n","‚úÖ Processed 4/2000 examples\n","‚úÖ Processed 5/2000 examples\n","‚úÖ Processed 6/2000 examples\n","‚úÖ Processed 7/2000 examples\n","‚úÖ Processed 8/2000 examples\n","‚úÖ Processed 9/2000 examples\n","‚úÖ Processed 10/2000 examples\n","‚úÖ Processed 11/2000 examples\n","‚úÖ Processed 12/2000 examples\n","‚úÖ Processed 13/2000 examples\n","‚úÖ Processed 14/2000 examples\n","‚úÖ Processed 15/2000 examples\n","‚úÖ Processed 16/2000 examples\n","‚úÖ Processed 17/2000 examples\n","‚úÖ Processed 18/2000 examples\n","‚úÖ Processed 19/2000 examples\n","‚úÖ Processed 20/2000 examples\n","‚úÖ Processed 21/2000 examples\n","‚úÖ Processed 22/2000 examples\n","‚úÖ Processed 23/2000 examples\n","‚úÖ Processed 24/2000 examples\n","‚úÖ Processed 25/2000 examples\n","‚úÖ Processed 26/2000 examples\n","‚úÖ Processed 27/2000 examples\n","‚úÖ Processed 28/2000 examples\n","‚úÖ Processed 29/2000 examples\n","‚úÖ Processed 30/2000 examples\n","‚úÖ Processed 31/2000 examples\n","‚úÖ Processed 32/2000 examples\n","‚úÖ Processed 33/2000 examples\n","‚úÖ Processed 34/2000 examples\n","‚úÖ Processed 35/2000 examples\n","‚úÖ Processed 36/2000 examples\n","‚úÖ Processed 37/2000 examples\n","‚úÖ Processed 38/2000 examples\n","‚úÖ Processed 39/2000 examples\n","‚úÖ Processed 40/2000 examples\n","‚úÖ Processed 41/2000 examples\n","‚úÖ Processed 42/2000 examples\n","‚úÖ Processed 43/2000 examples\n","‚úÖ Processed 45/2000 examples\n","‚úÖ Processed 46/2000 examples\n","‚úÖ Processed 47/2000 examples\n","‚úÖ Processed 48/2000 examples\n","‚úÖ Processed 49/2000 examples\n","‚úÖ Processed 50/2000 examples\n","‚úÖ Processed 51/2000 examples\n","‚úÖ Processed 52/2000 examples\n","‚úÖ Processed 53/2000 examples\n","‚úÖ Processed 54/2000 examples\n","‚úÖ Processed 55/2000 examples\n","‚úÖ Processed 56/2000 examples\n","‚úÖ Processed 57/2000 examples\n","‚úÖ Processed 58/2000 examples\n","‚úÖ Processed 59/2000 examples\n","‚úÖ Processed 60/2000 examples\n","‚úÖ Processed 61/2000 examples\n","‚úÖ Processed 62/2000 examples\n","‚úÖ Processed 63/2000 examples\n","‚úÖ Processed 64/2000 examples\n","‚úÖ Processed 65/2000 examples\n","‚úÖ Processed 66/2000 examples\n","‚úÖ Processed 67/2000 examples\n","‚úÖ Processed 68/2000 examples\n","‚úÖ Processed 69/2000 examples\n","‚úÖ Processed 70/2000 examples\n","‚úÖ Processed 71/2000 examples\n","‚úÖ Processed 72/2000 examples\n","‚úÖ Processed 73/2000 examples\n","‚úÖ Processed 74/2000 examples\n","‚úÖ Processed 75/2000 examples\n","‚úÖ Processed 76/2000 examples\n","‚úÖ Processed 77/2000 examples\n","‚úÖ Processed 78/2000 examples\n","‚úÖ Processed 79/2000 examples\n","‚úÖ Processed 80/2000 examples\n","‚úÖ Processed 81/2000 examples\n","‚úÖ Processed 82/2000 examples\n","‚úÖ Processed 83/2000 examples\n","‚úÖ Processed 84/2000 examples\n","‚úÖ Processed 85/2000 examples\n","‚úÖ Processed 86/2000 examples\n","‚úÖ Processed 87/2000 examples\n","‚úÖ Processed 88/2000 examples\n","‚úÖ Processed 89/2000 examples\n","‚úÖ Processed 90/2000 examples\n","‚úÖ Processed 91/2000 examples\n","‚úÖ Processed 92/2000 examples\n","‚úÖ Processed 93/2000 examples\n","‚úÖ Processed 94/2000 examples\n","‚úÖ Processed 95/2000 examples\n","‚úÖ Processed 96/2000 examples\n","‚úÖ Processed 97/2000 examples\n","‚úÖ Processed 98/2000 examples\n","‚úÖ Processed 99/2000 examples\n","‚úÖ Processed 100/2000 examples\n","‚úÖ Processed 101/2000 examples\n","‚úÖ Processed 102/2000 examples\n","‚úÖ Processed 103/2000 examples\n","‚úÖ Processed 104/2000 examples\n","‚úÖ Processed 105/2000 examples\n","‚úÖ Processed 106/2000 examples\n","‚úÖ Processed 107/2000 examples\n","‚úÖ Processed 108/2000 examples\n","‚úÖ Processed 109/2000 examples\n","‚úÖ Processed 110/2000 examples\n","‚úÖ Processed 111/2000 examples\n","‚úÖ Processed 112/2000 examples\n","‚úÖ Processed 113/2000 examples\n","‚úÖ Processed 114/2000 examples\n","‚úÖ Processed 115/2000 examples\n","‚úÖ Processed 116/2000 examples\n","‚úÖ Processed 117/2000 examples\n","‚úÖ Processed 118/2000 examples\n","‚úÖ Processed 119/2000 examples\n","‚úÖ Processed 120/2000 examples\n","‚úÖ Processed 121/2000 examples\n","‚úÖ Processed 122/2000 examples\n","‚úÖ Processed 123/2000 examples\n","‚úÖ Processed 124/2000 examples\n","‚úÖ Processed 125/2000 examples\n","‚úÖ Processed 126/2000 examples\n","‚úÖ Processed 127/2000 examples\n","‚úÖ Processed 128/2000 examples\n","‚úÖ Processed 129/2000 examples\n","‚úÖ Processed 130/2000 examples\n","‚úÖ Processed 131/2000 examples\n","‚úÖ Processed 132/2000 examples\n","‚úÖ Processed 133/2000 examples\n","‚úÖ Processed 134/2000 examples\n","‚úÖ Processed 135/2000 examples\n","‚úÖ Processed 136/2000 examples\n","‚úÖ Processed 137/2000 examples\n","‚úÖ Processed 138/2000 examples\n","‚úÖ Processed 139/2000 examples\n","‚úÖ Processed 140/2000 examples\n","‚úÖ Processed 141/2000 examples\n","‚úÖ Processed 142/2000 examples\n","‚úÖ Processed 143/2000 examples\n","‚úÖ Processed 144/2000 examples\n","‚úÖ Processed 145/2000 examples\n","‚úÖ Processed 146/2000 examples\n","‚úÖ Processed 147/2000 examples\n","‚úÖ Processed 148/2000 examples\n","‚úÖ Processed 149/2000 examples\n","‚úÖ Processed 150/2000 examples\n","‚úÖ Processed 151/2000 examples\n","‚úÖ Processed 152/2000 examples\n","‚úÖ Processed 153/2000 examples\n","‚úÖ Processed 154/2000 examples\n","‚úÖ Processed 155/2000 examples\n","‚úÖ Processed 156/2000 examples\n","‚úÖ Processed 157/2000 examples\n","‚úÖ Processed 158/2000 examples\n","‚úÖ Processed 159/2000 examples\n","‚úÖ Processed 160/2000 examples\n","‚úÖ Processed 161/2000 examples\n","‚úÖ Processed 162/2000 examples\n","‚úÖ Processed 163/2000 examples\n","‚úÖ Processed 164/2000 examples\n","‚úÖ Processed 165/2000 examples\n","‚úÖ Processed 166/2000 examples\n","‚úÖ Processed 167/2000 examples\n","‚úÖ Processed 168/2000 examples\n","‚úÖ Processed 169/2000 examples\n","‚úÖ Processed 170/2000 examples\n","‚úÖ Processed 171/2000 examples\n","‚úÖ Processed 172/2000 examples\n","‚úÖ Processed 173/2000 examples\n","‚úÖ Processed 174/2000 examples\n","‚úÖ Processed 175/2000 examples\n","‚úÖ Processed 176/2000 examples\n","‚úÖ Processed 177/2000 examples\n","‚úÖ Processed 178/2000 examples\n","‚úÖ Processed 179/2000 examples\n"]}],"source":["### Observation\n","### Due to the large prompt, each example takes about 40 seconds to run\n","\n","# !pip install bitsandbytes     ## Needed when training on Colab\n","import os\n","import pandas as pd\n","import torch\n","from tqdm.auto import tqdm\n","\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n",")\n","\n","# os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n","\n","GEMINI_SERVICE_ACCOUNT_PATH = os.environ.get(\"GEMINI_SERVICE_ACCOUNT_PATH\", \"../../../secrets/gemini-service-account.json\")\n","GOOGLE_CLOUD_PROJECT = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"newsjuice-123456\")\n","GOOGLE_CLOUD_REGION = ( \"us-central1\")\n","QWEN_MODEL_PATH = (\"Qwen/Qwen3-0.6B\")\n","QWEN_MAX_NEW_TOKENS = (\"512\")\n","QWEN_TEMPERATURE = (\"0.7\")\n","PODCAST_LOG_CSV =(\"podcast_results.csv\")\n","WANDB_PROJECT = \"newsjuice-finetune\"\n","\n","try:\n","    WANDB_API_KEY = os.environ.get(\"WANDB_API_KEY\")\n","except:\n","    with open(\"../../../secrets/wandb_api_key.txt\", \"r\") as f:\n","        WANDB_API_KEY = f.readline().strip()\n","\n","try:\n","    HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n","except:\n","    with open(\"../../../secrets/hf_token.txt\", \"r\") as f:\n","        HF_TOKEN = f.readline().strip()\n","def _infer_compute_dtype():\n","    \"\"\"Return the best available compute dtype for QLoRA training.\"\"\"\n","    if torch.cuda.is_available():\n","        try:\n","            major, _ = torch.cuda.get_device_capability()\n","            if major >= 8:\n","                return torch.bfloat16\n","        except Exception:\n","            pass\n","        return torch.float16\n","    return torch.float32\n","\n","\n","def run_predictions_on_finetuned_model(\n","    merged_model_path: str = \"qwen_qlora_podcast_merged\",\n","    csv_path: str = PODCAST_LOG_CSV,\n","    output_csv_path: str = None,\n","    max_new_tokens: int = 512,\n","    temperature: float = 0.7,\n","):\n","\n","    if not os.path.exists(csv_path):\n","        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n","\n","    if not os.path.exists(merged_model_path):\n","        raise FileNotFoundError(f\"Merged model path not found: {merged_model_path}\")\n","\n","    print(\"Evaluating Merged Fine-tuned Qwen Model\")\n","\n","    # Load the CSV\n","    df = pd.read_csv(csv_path)\n","    if \"chunk_text\" not in df.columns:\n","        raise ValueError(\"CSV must contain 'chunk_text' column\")\n","\n","    print(f\"Found {len(df)} examples to evaluate\")\n","\n","    # Load the merged model directly in bfloat16 for fast inference\n","    print(f\"\\nüöÄ Loading merged model from {merged_model_path}...\")\n","    model = AutoModelForCausalLM.from_pretrained(\n","        merged_model_path,\n","        token=HF_TOKEN,\n","        trust_remote_code=True,\n","        torch_dtype=torch.bfloat16,\n","        device_map=\"auto\" if torch.cuda.is_available() else None,\n","    )\n","    model.eval()\n","\n","    # Load tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(merged_model_path, token=HF_TOKEN, trust_remote_code=True)\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    print(\"Model and adapter loaded successfully\")\n","    print(\"Starting inference...\")\n","\n","    # Generate predictions for each example\n","    finetuned_outputs = []\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Inferencing\", unit=\"example\"):\n","\n","        chunk_text = row[\"chunk_text\"]\n","        if pd.isna(chunk_text) or str(chunk_text).strip() == \"\":\n","            finetuned_outputs.append(\"\")\n","            continue\n","\n","        # Build the prompt in the same format as training\n","        prompt = (\n","            f\"\"\"### Instruction:\\n\n","        You are a news podcast host. Based on the following relevant news articles, create an engaging podcast-style script to the user's question.\n","        The script must be no longer than 300 words under any circumstance. Make sure you dont go over the spesified word limit You should only include the text of the script. Do not include any of your thoughts or any sound effects.\n","\n","        Please create a podcast-style response that:\n","        1. Starts with a warm, engaging introduction\n","        2. Directly addresses the user's question using information from the articles\n","        3. Weaves together insights from the relevant news articles\n","        4. Maintains a conversational, podcast-like tone\n","        5. Ends with a thoughtful conclusion that stays within the 300-word limit\n","\n","        If the articles don't contain enough information to fully answer the question, acknowledge this and provide what insights you can while being transparent about limitations.\n","\n","        Format your response as if you're speaking directly to the listener in a podcast episode.\n","            ### Input:\\n{str(chunk_text).strip()}\\n\\n\n","            ### Response:\\n\"\"\"\n","        )\n","\n","        # Tokenize and generate\n","        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                **inputs,\n","                max_new_tokens=max_new_tokens,\n","                temperature=temperature,\n","                do_sample=temperature > 0,\n","                pad_token_id=tokenizer.pad_token_id,\n","                eos_token_id=tokenizer.eos_token_id,\n","            )\n","\n","        # Decode and extract only the generated response (not the prompt)\n","        full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        if \"### Response:\" in full_text:\n","            response = full_text.split(\"### Response:\")[1].strip()\n","        else:\n","            response = full_text[len(prompt):].strip()\n","\n","        finetuned_outputs.append(response)\n","\n","        tqdm.write(f\"‚úÖ Processed {idx + 1}/{len(df)} examples\")\n","\n","    # Add new column to dataframe\n","    df[\"finetuned Qwen 0.6\"] = finetuned_outputs\n","\n","    # Save to CSV\n","    output_path = output_csv_path if output_csv_path else csv_path\n","    df.to_csv(output_path, index=False)\n","\n","    print(\"Evaluation completed!\")\n","\n","    return output_path\n","\n","\n","run_predictions_on_finetuned_model()"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJ3rl+plD5/njlaejFa00U"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1013f37ce761413b8c9ac2d8c62d694b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1265ea30930e46d8ba0d21f480024019":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19ca0f4bebb14dfebcbd95ecaf97b0ef","placeholder":"‚Äã","style":"IPY_MODEL_f96c0aa2ce354b4b91da3443ebd8d56b","value":"‚Äá48/2000‚Äá[32:35&lt;19:54:18,‚Äá36.71s/example]"}},"13b199d3bcad48ddb206ac8a5b367e59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15034e482f1c449089c230632aba7752":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"19ca0f4bebb14dfebcbd95ecaf97b0ef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"246c3266bef34b2c9dc261ab9be2deb6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"272fb4f048f744729f4dbc3930913c70":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_67a437da6b7c473a996d148e9f36187b","max":239,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a080270963634a03b3363411aa17c268","value":239}},"39a7d683f8fa4715891e463ff270563d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40a532dbba774d308134391e0afbf7a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41b2f3ce15f045fc8da36780acfcae7a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42424c9e84ac42849d3b965a1a4a3e2d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51441de6edf94216883e4979e5396fba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13b199d3bcad48ddb206ac8a5b367e59","placeholder":"‚Äã","style":"IPY_MODEL_246c3266bef34b2c9dc261ab9be2deb6","value":"Inferencing:‚Äá‚Äá‚Äá2%"}},"62af37b141a24d239405468935631f4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41b2f3ce15f045fc8da36780acfcae7a","max":1503300328,"min":0,"orientation":"horizontal","style":"IPY_MODEL_15034e482f1c449089c230632aba7752","value":1503300328}},"67a437da6b7c473a996d148e9f36187b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72942c45b88244fb81d2f1b5bf8d6150":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"79f97ef43b9a460b9bc5a95a5e3534f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d74dcab889a497a8bf60b3c95003136":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d7f917bfa52c49d1847b5ef633011f7d","IPY_MODEL_272fb4f048f744729f4dbc3930913c70","IPY_MODEL_ecbcaeea24bd4dcda15a83f60a0c613f"],"layout":"IPY_MODEL_fd9e295ff6a042538eeefb157616a52e"}},"97bd86231e16443483d3ecf85e1e8e6b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97dbf73c71bf474eb7ead8eea3de267e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b0ebb476e2d34031b163201391834d55","IPY_MODEL_62af37b141a24d239405468935631f4d","IPY_MODEL_f6d28125c84d49bc8ceab33cd62c0fc3"],"layout":"IPY_MODEL_9a164efbd36742c9bfd0e1590b9e4404"}},"97eee52eba864ba3a5b9944cb99abec9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a164efbd36742c9bfd0e1590b9e4404":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a080270963634a03b3363411aa17c268":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b0ebb476e2d34031b163201391834d55":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42424c9e84ac42849d3b965a1a4a3e2d","placeholder":"‚Äã","style":"IPY_MODEL_1013f37ce761413b8c9ac2d8c62d694b","value":"model.safetensors:‚Äá100%"}},"be30e13e270b486f94d455ae3aa07cfd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccb50a2924374d29b28eef5fb04cec4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_79f97ef43b9a460b9bc5a95a5e3534f3","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_72942c45b88244fb81d2f1b5bf8d6150","value":48}},"d7f917bfa52c49d1847b5ef633011f7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40a532dbba774d308134391e0afbf7a5","placeholder":"‚Äã","style":"IPY_MODEL_97eee52eba864ba3a5b9944cb99abec9","value":"generation_config.json:‚Äá100%"}},"d8b4bd488dc8457a8c3d3906d5a61250":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_51441de6edf94216883e4979e5396fba","IPY_MODEL_ccb50a2924374d29b28eef5fb04cec4d","IPY_MODEL_1265ea30930e46d8ba0d21f480024019"],"layout":"IPY_MODEL_e06087e7699f4681bb3ca0d47b07f108"}},"e06087e7699f4681bb3ca0d47b07f108":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb6bd245c4d0471a963d5e99239f0339":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecbcaeea24bd4dcda15a83f60a0c613f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97bd86231e16443483d3ecf85e1e8e6b","placeholder":"‚Äã","style":"IPY_MODEL_39a7d683f8fa4715891e463ff270563d","value":"‚Äá239/239‚Äá[00:00&lt;00:00,‚Äá25.6kB/s]"}},"f6d28125c84d49bc8ceab33cd62c0fc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb6bd245c4d0471a963d5e99239f0339","placeholder":"‚Äã","style":"IPY_MODEL_be30e13e270b486f94d455ae3aa07cfd","value":"‚Äá1.50G/1.50G‚Äá[00:20&lt;00:00,‚Äá237MB/s]"}},"f96c0aa2ce354b4b91da3443ebd8d56b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd9e295ff6a042538eeefb157616a52e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}