{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO5BLRY0rxMESa5F3dgrPuc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9184ab7a24db43818230974c0c691e30":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c038e5fa24740c895d238ab8fb04b42","IPY_MODEL_48bdb5b111684feca6ac434a72bc937a","IPY_MODEL_48f58bd20f2e43388807d5bdc696d1c0"],"layout":"IPY_MODEL_039f2150289a4c018817b2450cae7dc5"}},"4c038e5fa24740c895d238ab8fb04b42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19cc721bdec7442aa2e580b09fbe1e0d","placeholder":"‚Äã","style":"IPY_MODEL_d4e64d1bc3524b73add99fd46cbf21aa","value":"Map:‚Äá100%"}},"48bdb5b111684feca6ac434a72bc937a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d48408932af4bf8931db5c888661a2c","max":1800,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de0853017bf3458384f8234fd096686c","value":1800}},"48f58bd20f2e43388807d5bdc696d1c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60270724a05540beacff1e3e9887352e","placeholder":"‚Äã","style":"IPY_MODEL_30189a31f6284a39bd2c79415f5bb58e","value":"‚Äá1800/1800‚Äá[00:05&lt;00:00,‚Äá325.18‚Äáexamples/s]"}},"039f2150289a4c018817b2450cae7dc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19cc721bdec7442aa2e580b09fbe1e0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4e64d1bc3524b73add99fd46cbf21aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d48408932af4bf8931db5c888661a2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de0853017bf3458384f8234fd096686c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"60270724a05540beacff1e3e9887352e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30189a31f6284a39bd2c79415f5bb58e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7347aa72ad5249f6a5ac13f07879ee65":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6753720fd5314529b75ea82a62455300","IPY_MODEL_9605198a8c674625a10f996d9ab77c24","IPY_MODEL_cc6d37f91a6149c3835b9f20a7345f9a"],"layout":"IPY_MODEL_2ede8872a5884881bd670585842d3135"}},"6753720fd5314529b75ea82a62455300":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42095f87e72a4cc0af41d33953292b82","placeholder":"‚Äã","style":"IPY_MODEL_445e8aaf4fca4f38908f1bc20a7af726","value":"Map:‚Äá100%"}},"9605198a8c674625a10f996d9ab77c24":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_07215432dd6442cbbc646476e4814fcc","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d4a8be3e73644ac8c47e4bf7ea89af0","value":200}},"cc6d37f91a6149c3835b9f20a7345f9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15ce8bc2d50440a790434aeb5e6b9acc","placeholder":"‚Äã","style":"IPY_MODEL_d622d8ea639249f2b98f53ecd9bf15af","value":"‚Äá200/200‚Äá[00:00&lt;00:00,‚Äá389.21‚Äáexamples/s]"}},"2ede8872a5884881bd670585842d3135":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42095f87e72a4cc0af41d33953292b82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"445e8aaf4fca4f38908f1bc20a7af726":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07215432dd6442cbbc646476e4814fcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d4a8be3e73644ac8c47e4bf7ea89af0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"15ce8bc2d50440a790434aeb5e6b9acc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d622d8ea639249f2b98f53ecd9bf15af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9184ab7a24db43818230974c0c691e30","4c038e5fa24740c895d238ab8fb04b42","48bdb5b111684feca6ac434a72bc937a","48f58bd20f2e43388807d5bdc696d1c0","039f2150289a4c018817b2450cae7dc5","19cc721bdec7442aa2e580b09fbe1e0d","d4e64d1bc3524b73add99fd46cbf21aa","1d48408932af4bf8931db5c888661a2c","de0853017bf3458384f8234fd096686c","60270724a05540beacff1e3e9887352e","30189a31f6284a39bd2c79415f5bb58e","7347aa72ad5249f6a5ac13f07879ee65","6753720fd5314529b75ea82a62455300","9605198a8c674625a10f996d9ab77c24","cc6d37f91a6149c3835b9f20a7345f9a","2ede8872a5884881bd670585842d3135","42095f87e72a4cc0af41d33953292b82","445e8aaf4fca4f38908f1bc20a7af726","07215432dd6442cbbc646476e4814fcc","4d4a8be3e73644ac8c47e4bf7ea89af0","15ce8bc2d50440a790434aeb5e6b9acc","d622d8ea639249f2b98f53ecd9bf15af"]},"id":"DSdHTjZoQHsQ","executionInfo":{"status":"ok","timestamp":1761794491885,"user_tz":240,"elapsed":4651109,"user":{"displayName":"khaled Aly","userId":"04673088304537721042"}},"outputId":"345ccf7a-e723-42c4-fdd6-8db45115ccf9"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing previous runs because reinit is set to 'default'."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/loss</td><td>‚ñà‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>0.2</td></tr><tr><td>train/global_step</td><td>90</td></tr><tr><td>train/grad_norm</td><td>0.26015</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>1.9579</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">eldritch-specter-1</strong> at: <a href='https://wandb.ai/xmokhtar-harvard-university/huggingface/runs/pwd6ttbd' target=\"_blank\">https://wandb.ai/xmokhtar-harvard-university/huggingface/runs/pwd6ttbd</a><br> View project at: <a href='https://wandb.ai/xmokhtar-harvard-university/huggingface' target=\"_blank\">https://wandb.ai/xmokhtar-harvard-university/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251030_015807-pwd6ttbd/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.22.2"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251030_020401-ytu5r7bv</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/xmokhtar-harvard-university/newsjuice-finetune/runs/ytu5r7bv' target=\"_blank\">strange-howl-1</a></strong> to <a href='https://wandb.ai/xmokhtar-harvard-university/newsjuice-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/xmokhtar-harvard-university/newsjuice-finetune' target=\"_blank\">https://wandb.ai/xmokhtar-harvard-university/newsjuice-finetune</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/xmokhtar-harvard-university/newsjuice-finetune/runs/ytu5r7bv' target=\"_blank\">https://wandb.ai/xmokhtar-harvard-university/newsjuice-finetune/runs/ytu5r7bv</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Starting QLoRA Fine-tuning for Qwen Model\n","Training Data: 2000 examples loaded from podcast_results.csv\n","Model: Qwen/Qwen3-0.6B\n","üìä Dataset split: 1800 train / 200 validation\n","\n","üî§ Tokenizing dataset...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1800 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9184ab7a24db43818230974c0c691e30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/200 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7347aa72ad5249f6a5ac13f07879ee65"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Tokenization complete: 1800 train examples, 200 validation examples\n","\n","‚öôÔ∏è  Compute dtype: torch.float16\n","\n","üì• Loading model with 4-bit quantization...\n","‚úÖ Model loaded successfully\n","\n","üéØ LoRA Configuration:\n","   - Rank (r): 64\n","   - Alpha: 16\n","   - Dropout: 0.05\n","   - Trainable params: 40,370,176 (9.70%)\n","   - Total params: 416,219,136\n","\n","============================================================\n","üèãÔ∏è  Starting training...\n","============================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1350' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1350/1350 1:17:13, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.840700</td>\n","      <td>1.773456</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.602400</td>\n","      <td>1.709547</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.529100</td>\n","      <td>1.703846</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Training completed!\n","\n","Saving model and tokenizer to: qwen_qlora_podcast\n","Model saved successfully!\n","\n","Fine-tuned adapter location: qwen_qlora_podcast\n"," Fine-tuning complete! You can now load the adapter with PEFT.\n","\n"]}],"source":["\n","###### IF bitsandbites WENT THROUGH THE INSTALLATION PROCESS,\n","###### MAKE SURE TO RESTART THE SESSION, OTHERWISE IT WILL NOT WORK\n","\n","# !pip install bitsandbytes     ## Needed when training on Colab\n","# !pip install psycopg          ## Needed when training on Colab\n","# !pip install wandb            ## Needed when training on Colab\n","import os\n","import pandas as pd\n","import torch\n","import wandb\n","from datasets import Dataset\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    DataCollatorForLanguageModeling,\n","    Trainer,\n","    TrainingArguments,\n","    pipeline,\n",")\n","from google.colab import userdata\n","\n","os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\n","os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n","\n","\n","# Configuration\n","GOOGLE_CLOUD_PROJECT = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"newsjuice-123456\")\n","GOOGLE_CLOUD_REGION = ( \"us-central1\")\n","QWEN_MODEL_PATH = (\"Qwen/Qwen3-0.6B\")\n","QWEN_MAX_NEW_TOKENS = (\"512\")\n","QWEN_TEMPERATURE = (\"0.7\")\n","PODCAST_LOG_CSV =(\"podcast_results.csv\")\n","WANDB_PROJECT = \"newsjuice-finetune\"\n","\n","WANDB_API_KEY = os.environ.get(\"WANDB_API_KEY\")\n","if not WANDB_API_KEY:\n","    try:\n","        with open(\"../../../secrets/wandb_api_key.txt\", \"r\") as f:\n","            WANDB_API_KEY = f.readline().strip()\n","    except FileNotFoundError:\n","        print(\"‚ö†Ô∏è WANDB API key file not found. Proceeding without wandb.\")\n","        WANDB_API_KEY = None\n","\n","HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n","if not HF_TOKEN:\n","    try:\n","        with open(\"../../../secrets/hf_token.txt\", \"r\") as f:\n","            HF_TOKEN = f.readline().strip()\n","    except FileNotFoundError:\n","        print(\"‚ö†Ô∏è Hugging Face token file not found. This may cause issues downloading the model.\")\n","        HF_TOKEN = None\n","\n","if WANDB_API_KEY:\n","    wandb.login(key=WANDB_API_KEY)\n","    wandb.init(project=WANDB_PROJECT)\n","\n","def _infer_compute_dtype():\n","    \"\"\"Return the best available compute dtype for QLoRA training.\"\"\"\n","    if torch.cuda.is_available():\n","        try:\n","            major, _ = torch.cuda.get_device_capability()\n","            if major >= 8:\n","                return torch.bfloat16\n","        except Exception:\n","            pass\n","        return torch.float16\n","    return torch.float32\n","\n","\n","def _build_prompt(article_snippet: str, podcast_text: str) -> str:\n","    \"\"\"Format training example into an instruction-style prompt.\"\"\"\n","\n","    article_snippet = \"\" if pd.isna(article_snippet) else str(article_snippet).strip()\n","    podcast_text = \"\" if pd.isna(podcast_text) else str(podcast_text).strip()\n","\n","    return (\n","        f\"\"\"### Instruction:\\n\n","        You are a news podcast host. Based on the following relevant news articles, create an engaging podcast-style script to the user's question.\n","        The script must be no longer than 300 words under any circumstance. Make sure you dont go over the spesified word limit You should only include the text of the script. Do not include any of your thoughts or any sound effects.\n","\n","        Please create a podcast-style response that:\n","        1. Starts with a warm, engaging introduction\n","        2. Directly addresses the user's question using information from the articles\n","        3. Weaves together insights from the relevant news articles\n","        4. Maintains a conversational, podcast-like tone\n","        5. Ends with a thoughtful conclusion that stays within the 300-word limit\n","\n","        If the articles don't contain enough information to fully answer the question, acknowledge this and provide what insights you can while being transparent about limitations.\n","\n","        Format your response as if you're speaking directly to the listener in a podcast episode.\n","        ### Input:\\n{article_snippet}\\n\\n\n","        ### Response:\\n{podcast_text}\"\"\"\n","    )\n","\n","def load_csv(csv_path):\n","    if not os.path.exists(csv_path):\n","        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n","\n","    df = pd.read_csv(csv_path)\n","    if \"gemini_podcast_text\" not in df.columns or \"chunk_text\" not in df.columns:\n","        raise ValueError(\"CSV must contain 'chunk_text' and 'gemini_podcast_text' columns\")\n","\n","    df[\"chunk_text\"] = df[\"chunk_text\"].dropna()\n","    df[\"gemini_podcast_text\"] = df[\"gemini_podcast_text\"].dropna()\n","    df = df[\n","        (df[\"chunk_text\"].str.strip() != \"\") & (df[\"gemini_podcast_text\"].str.strip() != \"\")\n","    ]\n","\n","    if df.empty:\n","        raise ValueError(\"No valid training rows found in the CSV after cleaning.\")\n","\n","    return df\n","\n","def split_train_test_and_tokanize(dataset, validation_split, max_seq_length):\n","        # Split into train and validation sets\n","    if validation_split > 0 and len(dataset) > 1:\n","        split_dataset = dataset.train_test_split(test_size=validation_split, seed=42)\n","        train_dataset = split_dataset[\"train\"]\n","        eval_dataset = split_dataset[\"test\"]\n","        print(f\"üìä Dataset split: {len(train_dataset)} train / {len(eval_dataset)} validation\")\n","    else:\n","        train_dataset = dataset\n","        eval_dataset = None\n","        print(f\"‚ö†Ô∏è  No validation split - training on all {len(train_dataset)} examples\")\n","\n","    tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_PATH, token=HF_TOKEN, trust_remote_code=True)\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"right\"\n","\n","    def tokenize_fn(batch):\n","        return tokenizer(\n","            batch[\"text\"],\n","            padding=False,\n","            truncation=True,\n","            max_length=max_seq_length,\n","        )\n","\n","    print(\"\\nüî§ Tokenizing dataset...\")\n","    tokenized_train = train_dataset.map(tokenize_fn, batched=True, remove_columns=train_dataset.column_names)\n","    tokenized_eval = eval_dataset.map(tokenize_fn, batched=True, remove_columns=eval_dataset.column_names) if eval_dataset else None\n","    print(f\"‚úÖ Tokenization complete: {len(tokenized_train)} train examples\" + (f\", {len(tokenized_eval)} validation examples\" if tokenized_eval else \"\"))\n","\n","    return tokenized_train, tokenized_eval, tokenizer\n","\n","\n","def finetune_qwen_with_qlora(\n","    csv_path: str = PODCAST_LOG_CSV,\n","    output_dir: str = (\"qwen_qlora_podcast\"),\n","    num_train_epochs: int = 3,\n","    per_device_train_batch_size: int = 1,\n","    gradient_accumulation_steps: int = 4,\n","    learning_rate: float = 2e-4,\n","    max_seq_length: int = 1024,\n","    warmup_steps: int = 20,\n","    logging_steps: int = 10,\n","    validation_split: float = 0.1,\n","):\n","\n","    df = load_csv(csv_path)\n","\n","    print(\"Starting QLoRA Fine-tuning for Qwen Model\")\n","    print(f\"Training Data: {len(df)} examples loaded from {csv_path}\")\n","    print(f\"Model: {QWEN_MODEL_PATH}\")\n","\n","    df[\"text\"] = df.apply(lambda row: _build_prompt(row[\"chunk_text\"], row[\"gemini_podcast_text\"]), axis=1)\n","\n","    dataset = Dataset.from_pandas(df[[\"text\"]])\n","    dataset = dataset.shuffle(seed=42)\n","\n","    tokenized_train, tokenized_eval, tokenizer = split_train_test_and_tokanize(dataset, validation_split, max_seq_length)\n","\n","    compute_dtype = _infer_compute_dtype()\n","    print(f\"\\n‚öôÔ∏è  Compute dtype: {compute_dtype}\")\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=compute_dtype,\n","    )\n","\n","    print(\"\\nüì• Loading model with 4-bit quantization...\")\n","    model = AutoModelForCausalLM.from_pretrained(\n","        QWEN_MODEL_PATH,\n","        token=HF_TOKEN,\n","        trust_remote_code=True,\n","        quantization_config=bnb_config,\n","        device_map=\"auto\" if torch.cuda.is_available() else None,\n","    )\n","    print(\"‚úÖ Model loaded successfully\")\n","\n","    model = prepare_model_for_kbit_training(model)\n","    lora_config = LoraConfig(\n","        r=64,\n","        lora_alpha=16,\n","        target_modules=[\n","            \"q_proj\",\n","            \"k_proj\",\n","            \"v_proj\",\n","            \"o_proj\",\n","            \"gate_proj\",\n","            \"up_proj\",\n","            \"down_proj\",\n","        ],\n","        lora_dropout=0.05,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","    )\n","    model = get_peft_model(model, lora_config)\n","    model.config.use_cache = False\n","\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    total_params = sum(p.numel() for p in model.parameters())\n","    print(f\"\\nüéØ LoRA Configuration:\")\n","    print(f\"   - Rank (r): {lora_config.r}\")\n","    print(f\"   - Alpha: {lora_config.lora_alpha}\")\n","    print(f\"   - Dropout: {lora_config.lora_dropout}\")\n","    print(f\"   - Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n","    print(f\"   - Total params: {total_params:,}\")\n","\n","    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    use_cuda = torch.cuda.is_available()\n","    use_bf16 = use_cuda and compute_dtype == torch.bfloat16\n","\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        num_train_epochs=num_train_epochs,\n","        per_device_train_batch_size=per_device_train_batch_size,\n","        gradient_accumulation_steps=gradient_accumulation_steps,\n","        learning_rate=learning_rate,\n","        warmup_steps=warmup_steps,\n","        logging_steps=logging_steps,\n","        save_strategy=\"epoch\",\n","        eval_strategy=\"epoch\" if tokenized_eval else \"no\",\n","        save_total_limit=2,\n","        load_best_model_at_end=True if tokenized_eval else False,\n","        metric_for_best_model=\"eval_loss\" if tokenized_eval else None,\n","        fp16=use_cuda and not use_bf16,\n","        bf16=use_bf16,\n","        optim=\"paged_adamw_8bit\",\n","        lr_scheduler_type=\"cosine\",\n","        weight_decay=0.01,\n","        report_to=\"wandb\" if WANDB_API_KEY else \"none\",\n","        remove_unused_columns=False,\n","        logging_first_step=True,\n","        disable_tqdm=False,\n","    )\n","\n","    print(f\"\\n{'='*60}\")\n","    print(\"üèãÔ∏è  Starting training...\")\n","    print(f\"{'='*60}\\n\")\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_train,\n","        eval_dataset=tokenized_eval,\n","        data_collator=data_collator,\n","    )\n","\n","    trainer.train()\n","    print(\"Training completed!\")\n","    print(f\"\\nSaving model and tokenizer to: {output_dir}\")\n","\n","    trainer.model.save_pretrained(output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","\n","    print(f\"Model saved successfully!\")\n","    print(f\"\\nFine-tuned adapter location: {output_dir}\")\n","    print(f\" Fine-tuning complete! You can now load the adapter with PEFT.\\n\")\n","\n","    return output_dir\n","\n","\n","\n","\n","\n","# Test the get_random_chunk function\n","if __name__ == \"__main__\":\n","    ### Run this function to generate training data points. it will take a while to run\n","    finetune_qwen_with_qlora()\n",""]},{"cell_type":"code","source":["!zip -r /content/qwen_adapter.zip /content/qwen_qlora_podcast/\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Caksz2-aKLHi","executionInfo":{"status":"ok","timestamp":1761794983411,"user_tz":240,"elapsed":37099,"user":{"displayName":"khaled Aly","userId":"04673088304537721042"}},"outputId":"569447a9-150c-463d-a176-157cf5d0f5d6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: content/qwen_qlora_podcast/ (stored 0%)\n","  adding: content/qwen_qlora_podcast/chat_template.jinja (deflated 76%)\n","  adding: content/qwen_qlora_podcast/tokenizer.json (deflated 81%)\n","  adding: content/qwen_qlora_podcast/tokenizer_config.json (deflated 90%)\n","  adding: content/qwen_qlora_podcast/vocab.json (deflated 61%)\n","  adding: content/qwen_qlora_podcast/added_tokens.json (deflated 68%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/ (stored 0%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/chat_template.jinja (deflated 76%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/tokenizer.json (deflated 81%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/tokenizer_config.json (deflated 90%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/vocab.json (deflated 61%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/added_tokens.json (deflated 68%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/trainer_state.json (deflated 79%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/scaler.pt (deflated 64%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/adapter_config.json (deflated 57%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/README.md (deflated 65%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/merges.txt (deflated 57%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/special_tokens_map.json (deflated 69%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/adapter_model.safetensors (deflated 7%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/rng_state.pth (deflated 26%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/training_args.bin (deflated 53%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/optimizer.pt (deflated 11%)\n","  adding: content/qwen_qlora_podcast/checkpoint-1350/scheduler.pt (deflated 62%)\n","  adding: content/qwen_qlora_podcast/adapter_config.json (deflated 57%)\n","  adding: content/qwen_qlora_podcast/README.md (deflated 65%)\n","  adding: content/qwen_qlora_podcast/merges.txt (deflated 57%)\n","  adding: content/qwen_qlora_podcast/special_tokens_map.json (deflated 69%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/ (stored 0%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/chat_template.jinja (deflated 76%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/tokenizer.json (deflated 81%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/tokenizer_config.json (deflated 90%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/vocab.json (deflated 61%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/added_tokens.json (deflated 68%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/trainer_state.json (deflated 79%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/scaler.pt (deflated 64%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/adapter_config.json (deflated 57%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/README.md (deflated 65%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/merges.txt (deflated 57%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/special_tokens_map.json (deflated 69%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/adapter_model.safetensors (deflated 7%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/rng_state.pth (deflated 26%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/training_args.bin (deflated 53%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/optimizer.pt (deflated 10%)\n","  adding: content/qwen_qlora_podcast/checkpoint-900/scheduler.pt (deflated 61%)\n","  adding: content/qwen_qlora_podcast/adapter_model.safetensors (deflated 7%)\n"]}]}]}